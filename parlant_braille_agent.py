"""
Parlant Braille Agent - Multimodal Braille-Native Conversational AI

This agent uses Parlant's guideline-based control to create a conversational
AI that natively understands 8-dot braille as a multimodal encoding substrate.
"""

import parlant.sdk as p
from braille_converter import BrailleConverter, BRAILLE_CHARS, BRAILLE_OFFSET
import numpy as np
import subprocess
import json

# Initialize braille converter
converter = BrailleConverter()

# Standard braille letter mapping
LETTER_MAP = {
    'a': '‚†Å', 'b': '‚†É', 'c': '‚†â', 'd': '‚†ô', 'e': '‚†ë', 'f': '‚†ã', 'g': '‚†õ',
    'h': '‚†ì', 'i': '‚†ä', 'j': '‚†ö', 'k': '‚†Ö', 'l': '‚†á', 'm': '‚†ç', 'n': '‚†ù',
    'o': '‚†ï', 'p': '‚†è', 'q': '‚†ü', 'r': '‚†ó', 's': '‚†é', 't': '‚†û', 'u': '‚†•',
    'v': '‚†ß', 'w': '‚†∫', 'x': '‚†≠', 'y': '‚†Ω', 'z': '‚†µ', ' ': '‚†Ä',
}
REVERSE_MAP = {v: k for k, v in LETTER_MAP.items()}


# ============================================================================
# BRAILLE TOOLS
# ============================================================================

@p.tool
async def encode_text_to_braille(context: p.ToolContext, text: str) -> p.ToolResult:
    """
    Encode plain text to standard braille letters.
    
    Args:
        text: The text to encode in braille
    
    Returns:
        Braille-encoded text using standard letter mapping
    """
    braille = ''.join(LETTER_MAP.get(c.lower(), '‚†ø') for c in text)
    return p.ToolResult(json.dumps({
        "original": text,
        "braille": braille,
        "length": len(braille),
        "encoding": "standard-letters"
    }))


@p.tool
async def decode_braille_to_text(context: p.ToolContext, braille: str) -> p.ToolResult:
    """
    Decode braille letters back to plain text.
    
    Args:
        braille: The braille text to decode
    
    Returns:
        Decoded plain text
    """
    text = ''.join(REVERSE_MAP.get(c, '?') for c in braille)
    return p.ToolResult(json.dumps({
        "braille": braille,
        "decoded": text,
        "encoding": "standard-letters"
    }))


@p.tool
async def encode_image_to_braille(
    context: p.ToolContext, 
    pattern: str = "gradient",
    size: int = 8
) -> p.ToolResult:
    """
    Generate a braille-encoded image pattern.
    
    Args:
        pattern: Type of pattern (gradient, checkerboard, circle, noise, solid_dark, solid_bright)
        size: Size of the image grid (default 8x8)
    
    Returns:
        Braille-encoded image representation
    """
    if pattern == "gradient":
        img = np.linspace(0, 255, size * size).reshape(size, size).astype(np.uint8)
    elif pattern == "checkerboard":
        img = np.zeros((size, size), dtype=np.uint8)
        img[::2, ::2] = 255
        img[1::2, 1::2] = 255
    elif pattern == "circle":
        y, x = np.ogrid[:size, :size]
        center = size // 2
        mask = (x - center)**2 + (y - center)**2 <= (size//3)**2
        img = np.zeros((size, size), dtype=np.uint8)
        img[mask] = 255
    elif pattern == "noise":
        img = np.random.randint(0, 256, (size, size), dtype=np.uint8)
    elif pattern == "solid_dark":
        img = np.zeros((size, size), dtype=np.uint8)
    elif pattern == "solid_bright":
        img = np.full((size, size), 255, dtype=np.uint8)
    else:
        img = np.linspace(0, 255, size * size).reshape(size, size).astype(np.uint8)
    
    # Convert to braille
    braille_lines = []
    for row in img:
        braille_row = ''.join(BRAILLE_CHARS[int(v)] for v in row)
        braille_lines.append(braille_row)
    
    braille_img = '\n'.join(braille_lines)
    
    return p.ToolResult(json.dumps({
        "pattern": pattern,
        "size": f"{size}x{size}",
        "braille": braille_img,
        "encoding": "pixel-intensity",
        "description": f"Each braille character represents pixel intensity 0-255"
    }))


@p.tool
async def interpret_braille_pattern(
    context: p.ToolContext,
    braille: str
) -> p.ToolResult:
    """
    Analyze a braille pattern and determine its modality and meaning.
    
    Args:
        braille: The braille pattern to interpret
    
    Returns:
        Analysis of the braille pattern including modality detection
    """
    # Analyze pattern characteristics
    chars = [c for c in braille if c in BRAILLE_CHARS or ord(c) >= BRAILLE_OFFSET]
    
    if not chars:
        return p.ToolResult(json.dumps({"error": "No valid braille characters found"}))
    
    # Check if it looks like text (uses sparse letter patterns)
    text_chars = set(LETTER_MAP.values())
    text_ratio = sum(1 for c in chars if c in text_chars) / len(chars)
    
    # Check intensity distribution
    intensities = [ord(c) - BRAILLE_OFFSET for c in chars if ord(c) >= BRAILLE_OFFSET]
    avg_intensity = np.mean(intensities) if intensities else 0
    intensity_std = np.std(intensities) if intensities else 0
    
    # Determine modality
    if text_ratio > 0.7:
        modality = "text"
        decoded = ''.join(REVERSE_MAP.get(c, '?') for c in chars)
        interpretation = f"Text content: '{decoded}'"
    elif '\n' in braille:
        modality = "image"
        lines = braille.strip().split('\n')
        interpretation = f"Image grid: {len(lines)} rows, appears to be a visual pattern"
    elif intensity_std > 50:
        modality = "audio"
        interpretation = "High variation suggests audio MFCC features"
    else:
        modality = "unknown"
        interpretation = "Pattern type unclear"
    
    return p.ToolResult(json.dumps({
        "modality": modality,
        "interpretation": interpretation,
        "stats": {
            "char_count": len(chars),
            "avg_intensity": round(avg_intensity, 2),
            "intensity_std": round(intensity_std, 2),
            "text_ratio": round(text_ratio, 2)
        }
    }))


@p.tool
async def generate_audio_braille(
    context: p.ToolContext,
    audio_type: str = "sine",
    length: int = 32
) -> p.ToolResult:
    """
    Generate braille-encoded audio features.
    
    Args:
        audio_type: Type of audio (sine, noise, chirp, speech, rhythm)
        length: Number of braille characters (time steps)
    
    Returns:
        Braille-encoded audio representation
    """
    if audio_type == "sine":
        values = np.sin(np.linspace(0, 4*np.pi, length)) * 50 + 128
    elif audio_type == "noise":
        values = np.random.randint(0, 256, length)
    elif audio_type == "chirp":
        values = np.linspace(50, 200, length)
    elif audio_type == "speech":
        # Simulate speech-like modulation
        values = np.sin(np.linspace(0, 8*np.pi, length)) * 30 + 128
        values += np.random.randn(length) * 10
    elif audio_type == "rhythm":
        # Alternating pattern
        values = np.array([200 if i % 4 < 2 else 50 for i in range(length)])
    else:
        values = np.sin(np.linspace(0, 4*np.pi, length)) * 50 + 128
    
    values = np.clip(values, 0, 255).astype(int)
    braille = ''.join(BRAILLE_CHARS[v] for v in values)
    
    return p.ToolResult(json.dumps({
        "audio_type": audio_type,
        "braille": braille,
        "length": length,
        "encoding": "mfcc-features",
        "description": f"Simulated {audio_type} audio encoded as braille"
    }))


@p.tool
async def query_multimodal_braille_model(
    context: p.ToolContext,
    query: str
) -> p.ToolResult:
    """
    Query the multimodal-braille Ollama model directly.
    
    Args:
        query: The query to send to the model
    
    Returns:
        Response from the multimodal braille model
    """
    try:
        result = subprocess.run(
            ["ollama", "run", "multimodal-braille", query],
            capture_output=True,
            text=True,
            timeout=60
        )
        return p.ToolResult(json.dumps({
            "query": query,
            "response": result.stdout.strip(),
            "model": "multimodal-braille"
        }))
    except subprocess.TimeoutExpired:
        return p.ToolResult(json.dumps({"error": "Model query timed out"}))
    except Exception as e:
        return p.ToolResult(json.dumps({"error": str(e)}))


@p.tool
async def create_video_sequence(
    context: p.ToolContext,
    motion_type: str = "left_to_right",
    frames: int = 4,
    size: int = 4
) -> p.ToolResult:
    """
    Generate a braille-encoded video sequence showing motion.
    
    Args:
        motion_type: Type of motion (left_to_right, fade_in, fade_out, expanding, flashing)
        frames: Number of frames
        size: Size of each frame grid
    
    Returns:
        Braille-encoded video sequence
    """
    frame_brailles = []
    
    for f in range(frames):
        if motion_type == "left_to_right":
            frame = np.zeros((size, size), dtype=np.uint8)
            col = f % size
            frame[:, col] = 255
        elif motion_type == "fade_in":
            intensity = int(255 * f / (frames - 1)) if frames > 1 else 255
            frame = np.full((size, size), intensity, dtype=np.uint8)
        elif motion_type == "fade_out":
            intensity = int(255 * (1 - f / (frames - 1))) if frames > 1 else 0
            frame = np.full((size, size), intensity, dtype=np.uint8)
        elif motion_type == "expanding":
            frame = np.zeros((size, size), dtype=np.uint8)
            radius = (f + 1) * size // frames
            center = size // 2
            for i in range(size):
                for j in range(size):
                    if abs(i - center) <= radius and abs(j - center) <= radius:
                        frame[i, j] = 255
        elif motion_type == "flashing":
            intensity = 255 if f % 2 == 0 else 0
            frame = np.full((size, size), intensity, dtype=np.uint8)
        else:
            frame = np.zeros((size, size), dtype=np.uint8)
        
        braille = ''.join(BRAILLE_CHARS[int(v)] for row in frame for v in row)
        frame_brailles.append(f"Frame {f+1}: {braille}")
    
    video = '\n'.join(frame_brailles)
    
    return p.ToolResult(json.dumps({
        "motion_type": motion_type,
        "frames": frames,
        "size": f"{size}x{size}",
        "video": video,
        "encoding": "frame-sequence"
    }))


# ============================================================================
# ADVANCED MULTIMODAL TOOLS
# ============================================================================

@p.tool
async def analyze_braille_semantics(
    context: p.ToolContext,
    braille: str,
    compare_to: str = ""
) -> p.ToolResult:
    """
    Perform deep semantic analysis on braille patterns including
    Hamming distance, dot density, structural similarity, and cross-modal inference.
    
    Args:
        braille: The braille pattern to analyze
        compare_to: Optional second pattern for comparison
    
    Returns:
        Comprehensive semantic analysis
    """
    def get_dot_pattern(char):
        """Extract 8-bit dot pattern from braille character"""
        if ord(char) >= BRAILLE_OFFSET:
            return ord(char) - BRAILLE_OFFSET
        return 0
    
    def popcount(n):
        """Count number of set bits"""
        return bin(n).count('1')
    
    def hamming_distance(a, b):
        """Calculate Hamming distance between two patterns"""
        return popcount(a ^ b)
    
    chars = [c for c in braille if ord(c) >= BRAILLE_OFFSET]
    if not chars:
        return p.ToolResult(json.dumps({"error": "No valid braille characters"}))
    
    patterns = [get_dot_pattern(c) for c in chars]
    
    # Structural analysis
    total_dots = sum(popcount(p) for p in patterns)
    max_dots = len(patterns) * 8
    density = total_dots / max_dots if max_dots > 0 else 0
    
    # Pattern distribution
    unique_patterns = len(set(patterns))
    entropy = unique_patterns / 256  # Normalized entropy
    
    # Sequence analysis
    transitions = sum(1 for i in range(len(patterns)-1) if patterns[i] != patterns[i+1])
    smoothness = 1 - (transitions / len(patterns)) if len(patterns) > 1 else 1
    
    # Gradient detection
    is_ascending = all(patterns[i] <= patterns[i+1] for i in range(len(patterns)-1))
    is_descending = all(patterns[i] >= patterns[i+1] for i in range(len(patterns)-1))
    
    analysis = {
        "length": len(chars),
        "total_dots": total_dots,
        "density": round(density, 3),
        "unique_patterns": unique_patterns,
        "entropy": round(entropy, 3),
        "smoothness": round(smoothness, 3),
        "is_gradient": is_ascending or is_descending,
        "gradient_direction": "ascending" if is_ascending else "descending" if is_descending else "none",
        "semantic_inference": ""
    }
    
    # Semantic inference
    if density < 0.2:
        analysis["semantic_inference"] = "Low density suggests sparse data: possibly text, silence, or dark image regions"
    elif density > 0.7:
        analysis["semantic_inference"] = "High density suggests rich data: possibly bright images, loud audio, or complex patterns"
    elif analysis["is_gradient"]:
        analysis["semantic_inference"] = "Gradient pattern detected: likely represents a visual gradient, audio fade, or temporal transition"
    elif smoothness < 0.3:
        analysis["semantic_inference"] = "High variation suggests noise, texture, or complex audio with many transients"
    else:
        analysis["semantic_inference"] = "Moderate complexity: could be structured text, patterned image, or tonal audio"
    
    # Comparison if provided
    if compare_to:
        compare_chars = [c for c in compare_to if ord(c) >= BRAILLE_OFFSET]
        if compare_chars:
            compare_patterns = [get_dot_pattern(c) for c in compare_chars]
            min_len = min(len(patterns), len(compare_patterns))
            total_hamming = sum(hamming_distance(patterns[i], compare_patterns[i]) for i in range(min_len))
            similarity = 1 - (total_hamming / (min_len * 8)) if min_len > 0 else 0
            analysis["comparison"] = {
                "hamming_distance": total_hamming,
                "similarity": round(similarity, 3),
                "interpretation": "Very similar" if similarity > 0.8 else "Somewhat similar" if similarity > 0.5 else "Different"
            }
    
    return p.ToolResult(json.dumps(analysis))


@p.tool
async def convert_between_modalities(
    context: p.ToolContext,
    braille: str,
    source_modality: str,
    target_modality: str
) -> p.ToolResult:
    """
    Reinterpret braille data from one modality to another.
    For example, treat text braille as if it were image data, or audio as video frames.
    
    Args:
        braille: The braille pattern
        source_modality: Original interpretation (text, image, audio)
        target_modality: New interpretation (text, image, audio, haptic)
    
    Returns:
        Cross-modal reinterpretation
    """
    chars = [c for c in braille if ord(c) >= BRAILLE_OFFSET or c in LETTER_MAP.values()]
    if not chars:
        return p.ToolResult(json.dumps({"error": "No valid braille characters"}))
    
    result = {
        "source_modality": source_modality,
        "target_modality": target_modality,
        "original": braille[:50] + "..." if len(braille) > 50 else braille,
        "reinterpretation": ""
    }
    
    if target_modality == "text":
        # Interpret as text
        decoded = ''.join(REVERSE_MAP.get(c, '?') for c in chars)
        result["reinterpretation"] = decoded
        result["note"] = "Interpreted braille patterns as letter encoding"
        
    elif target_modality == "image":
        # Interpret as image description
        intensities = [ord(c) - BRAILLE_OFFSET for c in chars if ord(c) >= BRAILLE_OFFSET]
        if intensities:
            avg = np.mean(intensities)
            std = np.std(intensities)
            result["reinterpretation"] = f"Visual: {len(intensities)} pixels, avg brightness {avg:.0f}/255, contrast {std:.0f}"
            if std < 20:
                result["visual_description"] = "Uniform/solid region"
            elif avg < 50:
                result["visual_description"] = "Dark region with some detail"
            elif avg > 200:
                result["visual_description"] = "Bright region with some detail"
            else:
                result["visual_description"] = "Mid-tone region with texture"
    
    elif target_modality == "audio":
        # Interpret as audio description
        intensities = [ord(c) - BRAILLE_OFFSET for c in chars if ord(c) >= BRAILLE_OFFSET]
        if intensities:
            # Treat as amplitude over time
            peak = max(intensities)
            rms = np.sqrt(np.mean(np.array(intensities)**2))
            zero_crossings = sum(1 for i in range(len(intensities)-1) 
                                if (intensities[i] < 128) != (intensities[i+1] < 128))
            result["reinterpretation"] = f"Audio: {len(intensities)} samples, peak {peak}, RMS {rms:.0f}"
            result["audio_description"] = f"Estimated frequency activity: {'high' if zero_crossings > len(intensities)//4 else 'low'}"
    
    elif target_modality == "haptic":
        # Convert to haptic vibration pattern
        intensities = [ord(c) - BRAILLE_OFFSET for c in chars if ord(c) >= BRAILLE_OFFSET]
        if intensities:
            # Map to vibration intensities and durations
            haptic_pattern = []
            for i, intensity in enumerate(intensities[:16]):  # Limit to 16 pulses
                duration = 50 + (intensity // 5)  # 50-100ms
                strength = intensity // 25  # 0-10 scale
                haptic_pattern.append({"pulse": i+1, "duration_ms": duration, "strength": strength})
            result["reinterpretation"] = f"Haptic sequence: {len(haptic_pattern)} pulses"
            result["haptic_pattern"] = haptic_pattern
            result["note"] = "Can be sent to vibration motors or braille display"
    
    return p.ToolResult(json.dumps(result))


@p.tool
async def generate_braille_art(
    context: p.ToolContext,
    art_type: str = "wave",
    width: int = 32,
    height: int = 8
) -> p.ToolResult:
    """
    Generate artistic braille patterns for visual or tactile display.
    
    Args:
        art_type: Type of art (wave, spiral, mandala, text_gradient, noise_field, heartbeat)
        width: Width in braille characters
        height: Height in braille characters
    
    Returns:
        Braille art pattern
    """
    art = np.zeros((height, width), dtype=np.uint8)
    
    if art_type == "wave":
        for y in range(height):
            for x in range(width):
                art[y, x] = int(128 + 127 * np.sin(x * 0.3 + y * 0.5))
    
    elif art_type == "spiral":
        cx, cy = width // 2, height // 2
        for y in range(height):
            for x in range(width):
                dx, dy = x - cx, y - cy
                angle = np.arctan2(dy, dx)
                dist = np.sqrt(dx*dx + dy*dy)
                art[y, x] = int(128 + 127 * np.sin(angle * 3 + dist * 0.5))
    
    elif art_type == "mandala":
        cx, cy = width // 2, height // 2
        for y in range(height):
            for x in range(width):
                dx, dy = x - cx, y - cy
                dist = np.sqrt(dx*dx + dy*dy)
                angle = np.arctan2(dy, dx)
                art[y, x] = int(128 + 127 * np.sin(dist * 0.8) * np.cos(angle * 6))
    
    elif art_type == "text_gradient":
        # Gradient that could represent text fading
        for y in range(height):
            for x in range(width):
                art[y, x] = int(255 * x / width)
    
    elif art_type == "noise_field":
        art = np.random.randint(0, 256, (height, width), dtype=np.uint8)
    
    elif art_type == "heartbeat":
        # ECG-like pattern
        for y in range(height):
            baseline = height // 2
            for x in range(width):
                # Create heartbeat spikes
                phase = (x % 8) / 8
                if phase < 0.1:
                    val = 50
                elif phase < 0.2:
                    val = 250  # Spike up
                elif phase < 0.3:
                    val = 20   # Spike down
                elif phase < 0.4:
                    val = 180  # Recovery
                else:
                    val = 128  # Baseline
                if abs(y - baseline) < 2:
                    art[y, x] = val
                else:
                    art[y, x] = 0
    else:
        # Default: random
        art = np.random.randint(0, 256, (height, width), dtype=np.uint8)
    
    # Convert to braille
    lines = []
    for row in art:
        line = ''.join(BRAILLE_CHARS[int(v)] for v in row)
        lines.append(line)
    
    braille_art = '\n'.join(lines)
    
    return p.ToolResult(json.dumps({
        "art_type": art_type,
        "dimensions": f"{width}x{height}",
        "braille_art": braille_art,
        "total_chars": width * height,
        "note": "Can be displayed visually or rendered on a refreshable braille display"
    }))


@p.tool
async def simulate_braille_hardware(
    context: p.ToolContext,
    braille: str,
    device_type: str = "display"
) -> p.ToolResult:
    """
    Simulate how braille would be rendered on physical hardware.
    
    Args:
        braille: The braille pattern to render
        device_type: Type of device (display, embosser, haptic_glove, audio_reader)
    
    Returns:
        Hardware simulation details
    """
    chars = [c for c in braille if ord(c) >= BRAILLE_OFFSET or c in LETTER_MAP.values()]
    if not chars:
        return p.ToolResult(json.dumps({"error": "No valid braille characters"}))
    
    result = {
        "device_type": device_type,
        "input_length": len(chars),
        "simulation": {}
    }
    
    if device_type == "display":
        # Refreshable braille display simulation
        cells_per_line = 40  # Standard display width
        lines_needed = (len(chars) + cells_per_line - 1) // cells_per_line
        result["simulation"] = {
            "display_type": "Refreshable Braille Display",
            "cells_per_line": cells_per_line,
            "lines_needed": lines_needed,
            "total_pin_actuations": sum(bin(ord(c) - BRAILLE_OFFSET).count('1') for c in chars if ord(c) >= BRAILLE_OFFSET),
            "estimated_read_time_seconds": len(chars) * 0.5,  # ~0.5s per cell for tactile reading
            "power_consumption_mw": len(chars) * 2  # ~2mW per cell actuation
        }
    
    elif device_type == "embosser":
        # Braille embosser simulation
        result["simulation"] = {
            "device_type": "Braille Embosser",
            "paper_size": "Letter (8.5x11 in)",
            "chars_per_line": 42,
            "lines_per_page": 25,
            "pages_needed": max(1, len(chars) // (42 * 25)),
            "embossing_time_seconds": len(chars) * 0.1,
            "dots_to_emboss": sum(bin(ord(c) - BRAILLE_OFFSET).count('1') for c in chars if ord(c) >= BRAILLE_OFFSET)
        }
    
    elif device_type == "haptic_glove":
        # Haptic glove simulation
        patterns = [ord(c) - BRAILLE_OFFSET for c in chars[:20] if ord(c) >= BRAILLE_OFFSET]  # First 20
        vibration_sequence = []
        for i, p in enumerate(patterns):
            # Map 8 dots to 8 fingertip actuators
            actuators = [bool(p & (1 << bit)) for bit in range(8)]
            vibration_sequence.append({
                "step": i + 1,
                "actuators": actuators,
                "duration_ms": 200
            })
        result["simulation"] = {
            "device_type": "8-Actuator Haptic Glove",
            "sequence_length": len(vibration_sequence),
            "total_duration_ms": len(vibration_sequence) * 200,
            "first_5_patterns": vibration_sequence[:5]
        }
    
    elif device_type == "audio_reader":
        # Screen reader / audio description
        text_chars = [REVERSE_MAP.get(c, None) for c in chars]
        readable_text = ''.join(c if c else '?' for c in text_chars)
        result["simulation"] = {
            "device_type": "Screen Reader / TTS",
            "decoded_text": readable_text[:100],
            "word_count": len(readable_text.split()),
            "estimated_speech_seconds": len(readable_text.split()) * 0.4,  # ~150 WPM
            "phonemes": len(readable_text)
        }
    
    return p.ToolResult(json.dumps(result))


@p.tool
async def braille_compression_analysis(
    context: p.ToolContext,
    data_type: str,
    sample_text: str = ""
) -> p.ToolResult:
    """
    Analyze how different data types compress into braille representation.
    Demonstrates the semantic compression properties of braille encoding.
    
    Args:
        data_type: Type of data to analyze (text, image, audio, code)
        sample_text: Optional sample text for text analysis
    
    Returns:
        Compression analysis and semantic density metrics
    """
    result = {
        "data_type": data_type,
        "analysis": {}
    }
    
    if data_type == "text":
        text = sample_text or "The quick brown fox jumps over the lazy dog"
        braille = ''.join(LETTER_MAP.get(c.lower(), '‚†ø') for c in text)
        
        result["analysis"] = {
            "original_text": text,
            "braille": braille,
            "original_bytes": len(text.encode('utf-8')),
            "braille_bytes": len(braille.encode('utf-8')),
            "compression_ratio": round(len(text.encode('utf-8')) / len(braille.encode('utf-8')), 3),
            "semantic_density": "1.0 (lossless - each letter preserved)",
            "information_preserved": "100% - bijective mapping"
        }
    
    elif data_type == "image":
        # Simulate 256x256 grayscale image
        original_pixels = 256 * 256
        original_bytes = original_pixels  # 1 byte per pixel
        
        # Compressed to 32x32 braille grid
        braille_chars = 32 * 32
        braille_bytes = braille_chars * 3  # UTF-8 braille is 3 bytes
        
        result["analysis"] = {
            "original_resolution": "256x256 grayscale",
            "braille_resolution": "32x32 braille grid",
            "original_bytes": original_bytes,
            "braille_bytes": braille_bytes,
            "spatial_compression": "64:1 (8x8 pixels ‚Üí 1 braille cell)",
            "semantic_density": "0.85 (lossy - preserves structure, loses fine detail)",
            "information_preserved": "~15% raw, ~85% semantic (edges, shapes, gradients)"
        }
    
    elif data_type == "audio":
        # Simulate 1 second of 16kHz audio
        original_samples = 16000
        original_bytes = original_samples * 2  # 16-bit audio
        
        # Compressed to 32 MFCC frames √ó 13 coefficients
        braille_chars = 32 * 13
        braille_bytes = braille_chars * 3
        
        result["analysis"] = {
            "original_format": "1 second @ 16kHz, 16-bit",
            "braille_format": "32 frames √ó 13 MFCC coefficients",
            "original_bytes": original_bytes,
            "braille_bytes": braille_bytes,
            "compression_ratio": round(original_bytes / braille_bytes, 1),
            "semantic_density": "0.75 (lossy - preserves spectral envelope, loses phase)",
            "information_preserved": "~4% raw, ~75% semantic (pitch, timbre, rhythm)"
        }
    
    elif data_type == "code":
        code = sample_text or "def hello():\n    return 'world'"
        braille = ''.join(LETTER_MAP.get(c.lower(), '‚†ø') for c in code)
        
        # Count semantic tokens
        tokens = ['def', 'return', '(', ')', ':', "'"]
        token_count = sum(code.count(t) for t in tokens)
        
        result["analysis"] = {
            "original_code": code,
            "braille": braille,
            "original_bytes": len(code.encode('utf-8')),
            "braille_bytes": len(braille.encode('utf-8')),
            "semantic_tokens": token_count,
            "semantic_density": "1.0 (lossless for ASCII subset)",
            "note": "Code structure fully preserved in braille"
        }
    
    return p.ToolResult(json.dumps(result))


# ============================================================================
# PARLANT AGENT SETUP
# ============================================================================

async def main():
    # Use Ollama as the NLP backend (local LLM)
    async with p.Server(nlp_service=p.NLPServices.ollama) as server:
        # Create the braille agent
        agent = await server.create_agent(
            name="BrailleNative",
            description="A multimodal AI that natively understands 8-dot braille as a universal encoding for text, images, audio, and video."
        )
        
        print("üî§ Creating BrailleNative agent...")
        
        # ================================================================
        # GUIDELINES - Control agent behavior with natural language
        # ================================================================
        
        # Text encoding/decoding
        await agent.create_guideline(
            condition="User wants to encode text to braille",
            action="Use the encode_text_to_braille tool to convert their text, then explain the braille output",
            tools=[encode_text_to_braille]
        )
        
        await agent.create_guideline(
            condition="User provides braille text to decode or asks what braille means",
            action="Use decode_braille_to_text to decode it, then explain the meaning",
            tools=[decode_braille_to_text]
        )
        
        # Image generation and interpretation
        await agent.create_guideline(
            condition="User wants to see or generate a braille image pattern",
            action="Use encode_image_to_braille to generate the pattern, explain what it represents visually",
            tools=[encode_image_to_braille]
        )
        
        # Audio generation
        await agent.create_guideline(
            condition="User asks about audio or sound in braille",
            action="Use generate_audio_braille to create audio features, explain how braille encodes spectral characteristics",
            tools=[generate_audio_braille]
        )
        
        # Video/motion
        await agent.create_guideline(
            condition="User asks about video, motion, or animation in braille",
            action="Use create_video_sequence to generate frames, explain the motion pattern",
            tools=[create_video_sequence]
        )
        
        # Pattern interpretation
        await agent.create_guideline(
            condition="User provides a braille pattern and asks what it represents",
            action="Use interpret_braille_pattern to analyze it, then explain the modality and meaning",
            tools=[interpret_braille_pattern]
        )
        
        # Deep queries to the multimodal model
        await agent.create_guideline(
            condition="User asks a complex question about braille semantics or needs deep interpretation",
            action="Use query_multimodal_braille_model to get a detailed response from the specialized model",
            tools=[query_multimodal_braille_model]
        )
        
        # General braille education
        await agent.create_guideline(
            condition="User asks about braille basics or how braille encoding works",
            action="Explain that 8-dot braille uses Unicode U+2800-U+28FF (256 patterns) and can encode text (letters), images (pixel intensity), audio (MFCC features), and video (frame sequences). Offer to demonstrate with examples."
        )
        
        # Multimodal explanation
        await agent.create_guideline(
            condition="User asks why braille is used for images/audio/video",
            action="Explain that braille provides a universal 256-symbol vocabulary that maps naturally to byte values (0-255), making it ideal for encoding any data type. This enables 'braille-native cognition' where AI can reason directly in braille space."
        )
        
        # Advanced semantic analysis
        await agent.create_guideline(
            condition="User wants to analyze braille patterns, compare patterns, or understand semantic properties",
            action="Use analyze_braille_semantics to perform deep analysis including Hamming distance, entropy, and semantic inference",
            tools=[analyze_braille_semantics]
        )
        
        # Cross-modal conversion
        await agent.create_guideline(
            condition="User wants to convert braille between modalities or reinterpret data",
            action="Use convert_between_modalities to show how the same braille can be interpreted as text, image, audio, or haptic feedback",
            tools=[convert_between_modalities]
        )
        
        # Braille art generation
        await agent.create_guideline(
            condition="User wants artistic braille patterns, visual art, or creative displays",
            action="Use generate_braille_art to create beautiful patterns like waves, spirals, mandalas, or heartbeats",
            tools=[generate_braille_art]
        )
        
        # Hardware simulation
        await agent.create_guideline(
            condition="User asks about braille hardware, displays, embossers, or haptic devices",
            action="Use simulate_braille_hardware to show how braille would render on physical devices",
            tools=[simulate_braille_hardware]
        )
        
        # Compression analysis
        await agent.create_guideline(
            condition="User asks about compression, information density, or how much data braille can represent",
            action="Use braille_compression_analysis to demonstrate semantic compression properties",
            tools=[braille_compression_analysis]
        )
        
        print("‚úÖ BrailleNative agent created with multimodal guidelines!")
        print("üåê Test playground ready at http://localhost:8800")
        print("\n" + "="*60)
        print("EXAMPLE QUERIES - From Simple to Advanced")
        print("="*60)
        print("\nüìù TEXT ENCODING:")
        print("  ‚Ä¢ 'Encode the phrase artificial intelligence in braille'")
        print("  ‚Ä¢ 'Decode ‚†Å‚†ó‚†û‚†ä‚†ã‚†ä‚†â‚†ä‚†Å‚†á ‚†ä‚†ù‚†û‚†ë‚†á‚†á‚†ä‚†õ‚†ë‚†ù‚†â‚†ë and explain each letter'")
        print("\nüñºÔ∏è IMAGE PATTERNS:")
        print("  ‚Ä¢ 'Generate a 16x16 mandala pattern in braille for tactile art'")
        print("  ‚Ä¢ 'Create a spiral braille pattern and explain how it maps to visual perception'")
        print("  ‚Ä¢ 'Show me a heartbeat/ECG pattern encoded in braille'")
        print("\nüîä AUDIO REPRESENTATION:")
        print("  ‚Ä¢ 'Generate a chirp sound (rising frequency) as braille MFCC features'")
        print("  ‚Ä¢ 'What would speech vs music look like in braille encoding?'")
        print("  ‚Ä¢ 'Create a rhythmic drum pattern and convert it to haptic feedback'")
        print("\nüé¨ VIDEO/MOTION:")
        print("  ‚Ä¢ 'Create an 8-frame video of a shape expanding from the center'")
        print("  ‚Ä¢ 'Generate a fade-in transition as braille frames'")
        print("\nüî¨ SEMANTIC ANALYSIS:")
        print("  ‚Ä¢ 'Analyze the semantic properties of ‚†Ä‚†Å‚†É‚†á‚†è‚†ü‚†ø‚£ø - is it text, image, or audio?'")
        print("  ‚Ä¢ 'Compare these two patterns and calculate their similarity: ‚†ì‚†ë‚†á‚†á‚†ï vs ‚†ì‚†ë‚†á‚†è‚†ï'")
        print("  ‚Ä¢ 'What is the information density of braille-encoded images vs audio?'")
        print("\nüîÑ CROSS-MODAL:")
        print("  ‚Ä¢ 'Take this text braille ‚†ì‚†ë‚†á‚†á‚†ï and reinterpret it as if it were image data'")
        print("  ‚Ä¢ 'Convert this pattern to haptic vibration commands for a glove'")
        print("\nüñ®Ô∏è HARDWARE SIMULATION:")
        print("  ‚Ä¢ 'How would ‚†ì‚†ë‚†á‚†á‚†ï ‚†∫‚†ï‚†ó‚†á‚†ô render on a 40-cell refreshable braille display?'")
        print("  ‚Ä¢ 'Simulate embossing a braille document - how many dots and pages?'")
        print("  ‚Ä¢ 'Convert this pattern to an 8-actuator haptic glove sequence'")
        print("\nüìä COMPRESSION:")
        print("  ‚Ä¢ 'Analyze how a 256x256 image compresses into braille representation'")
        print("  ‚Ä¢ 'What percentage of audio information is preserved in braille MFCC encoding?'")
        print("  ‚Ä¢ 'Compare semantic density of text vs image vs audio in braille'")
        print("\n" + "="*60)
        print("Press Ctrl+C to stop the server.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNEDH1Ayc6Do"
      },
      "source": [
        "# ðŸ”¤ Braille-Native Cognition: LoRA Fine-Tuning\n",
        "\n",
        "Train a model to **think in 8-dot braille** - understanding text, images, audio, video, and binary files as braille patterns.\n",
        "\n",
        "**Novel contribution:** First LLM trained to recognize binary file signatures (ZIP, PNG, etc.) as braille.\n",
        "\n",
        "## Setup\n",
        "1. Runtime â†’ Change runtime type â†’ **T4 GPU** (free) or **A100** (Colab Pro)\n",
        "2. Run all cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2nEsm95Nc6Dp"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q unsloth transformers datasets peft accelerate bitsandbytes trl\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnAWS5FEc6Dq"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOr8FonUc6Dq"
      },
      "source": [
        "## Training Data\n",
        "\n",
        "133 examples covering:\n",
        "- Text â†” Braille encoding/decoding\n",
        "- Binary file signatures (ZIP, PNG, JPEG, PDF, etc.)\n",
        "- Multimodal understanding (images, audio as braille)\n",
        "- Tutoring scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjiPMu54c6Dq"
      },
      "outputs": [],
      "source": [
        "# Download training data from GitHub\n",
        "!wget -q https://raw.githubusercontent.com/elevate-foundry/braille-multimodal-substrate/main/lora_training/braille_alpaca.json\n",
        "\n",
        "import json\n",
        "with open('braille_alpaca.json', 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(training_data)} training examples\")\n",
        "print(f\"\\nExample:\")\n",
        "print(json.dumps(training_data[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bV1nuT3c6Dq"
      },
      "outputs": [],
      "source": [
        "# Format for training\n",
        "def format_prompt(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    output = example[\"output\"]\n",
        "\n",
        "    if input_text:\n",
        "        return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "\n",
        "# Create dataset\n",
        "from datasets import Dataset\n",
        "formatted_data = [{\"text\": format_prompt(ex)} for ex in training_data]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "print(f\"Dataset ready: {len(dataset)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR_zY7ndc6Dq"
      },
      "source": [
        "## Load Model with Unsloth (2x faster training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMo9IeWic6Dq"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Choose model size based on GPU\n",
        "# T4 (16GB): Use 3B or smaller\n",
        "# A100 (40GB): Can use 7B+\n",
        "\n",
        "max_seq_length = 2048\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # 3B params, 4-bit quantized\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pxWbEC_c6Dq"
      },
      "outputs": [],
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u383ZuQOc6Dq"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1onQq34-c6Dq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./braille_lora_output\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs=5,  # More epochs for better learning\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=42,\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No-l37mVc6Dq"
      },
      "source": [
        "## Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlhbJdThc6Dq"
      },
      "outputs": [],
      "source": [
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"What is the braille pattern for the letter 'h'?\",\n",
        "    \"What file type does this braille header indicate: â¡â¡‹â ƒâ „\",\n",
        "    \"Decode this braille: â “â ‘â ‡â ‡â •\",\n",
        "    \"How can braille represent any binary file?\",\n",
        "    \"Explain the structure of a ZIP file in braille encoding\",\n",
        "]\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Q: {prompt}\")\n",
        "\n",
        "    formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"### Response:\")[-1].strip()\n",
        "    print(f\"A: {response[:300]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8FOKCNZc6Dq"
      },
      "source": [
        "## Save & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80z0wo7oc6Dr"
      },
      "outputs": [],
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"braille_lora_adapters\")\n",
        "tokenizer.save_pretrained(\"braille_lora_adapters\")\n",
        "print(\"LoRA adapters saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfezK2MOc6Dr"
      },
      "outputs": [],
      "source": [
        "# Save merged model (for Ollama)\n",
        "model.save_pretrained_merged(\"braille_merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "print(\"Merged model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9L-xRpBc6Dr"
      },
      "outputs": [],
      "source": [
        "# Convert to GGUF for Ollama\n",
        "# This creates a file you can use with: ollama create braille-native -f Modelfile\n",
        "\n",
        "model.save_pretrained_gguf(\"braille_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "print(\"GGUF model saved! Download braille_gguf/ and use with Ollama.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hYD3CK2c6Dr"
      },
      "outputs": [],
      "source": [
        "# Optional: Push to Hugging Face Hub\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"your_token\")\n",
        "# model.push_to_hub(\"your-username/braille-native-3b-lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF6JUwLrc6Dr"
      },
      "source": [
        "## Download Files\n",
        "\n",
        "After training, download:\n",
        "1. `braille_gguf/` - For Ollama\n",
        "2. `braille_lora_adapters/` - LoRA weights only\n",
        "\n",
        "To use with Ollama:\n",
        "```bash\n",
        "# Create Modelfile\n",
        "echo 'FROM ./braille_gguf/model.gguf' > Modelfile\n",
        "echo 'SYSTEM \"You are a Braille-Native AI...\"' >> Modelfile\n",
        "\n",
        "# Create model\n",
        "ollama create braille-native -f Modelfile\n",
        "\n",
        "# Test\n",
        "ollama run braille-native \"Decode: â “â ‘â ‡â ‡â •\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXrg6XeMc6Dr"
      },
      "outputs": [],
      "source": [
        "# Zip for download\n",
        "!zip -r braille_gguf.zip braille_gguf/\n",
        "!zip -r braille_lora_adapters.zip braille_lora_adapters/\n",
        "\n",
        "from google.colab import files\n",
        "# files.download('braille_gguf.zip')  # Uncomment to download"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}